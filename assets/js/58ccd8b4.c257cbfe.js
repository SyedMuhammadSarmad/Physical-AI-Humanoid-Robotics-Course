"use strict";(globalThis.webpackChunkdocs=globalThis.webpackChunkdocs||[]).push([[3836],{5895:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>d,frontMatter:()=>s,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"part2/chapter6","title":"Chapter 6: Reinforcement Learning for Physical AI","description":"Reinforcement Learning (RL) is a powerful paradigm in machine learning where an agent learns to make decisions by interacting with an environment. The agent learns to take actions that maximize a cumulative reward signal, without being explicitly told which actions to take. This makes RL particularly well-suited for robotics, where agents must learn to operate in complex, dynamic, and uncertain environments.","source":"@site/docs/part2/chapter6.md","sourceDirName":"part2","slug":"/part2/chapter6","permalink":"/Physical-AI-Humanoid-Robotics-Course/docs/part2/chapter6","draft":false,"unlisted":false,"editUrl":"https://github.com/SyedMuhammadSarmad/Physical-AI-Humanoid-Robotics-Course/tree/main/docs/docs/part2/chapter6.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 5: Computer Vision in Robotics","permalink":"/Physical-AI-Humanoid-Robotics-Course/docs/part2/chapter5"},"next":{"title":"Chapter 7: Neural Networks for Robotic Control","permalink":"/Physical-AI-Humanoid-Robotics-Course/docs/part2/chapter7"}}');var i=r(4848),a=r(8453);const s={sidebar_position:2},o="Chapter 6: Reinforcement Learning for Physical AI",l={},c=[{value:"The Reinforcement Learning Framework",id:"the-reinforcement-learning-framework",level:2},{value:"Q-Learning",id:"q-learning",level:2},{value:"Code Example: Simple Q-Learning in Python",id:"code-example-simple-q-learning-in-python",level:3},{value:"Deep Reinforcement Learning",id:"deep-reinforcement-learning",level:2},{value:"References",id:"references",level:3}];function h(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"chapter-6-reinforcement-learning-for-physical-ai",children:"Chapter 6: Reinforcement Learning for Physical AI"})}),"\n",(0,i.jsx)(n.p,{children:"Reinforcement Learning (RL) is a powerful paradigm in machine learning where an agent learns to make decisions by interacting with an environment. The agent learns to take actions that maximize a cumulative reward signal, without being explicitly told which actions to take. This makes RL particularly well-suited for robotics, where agents must learn to operate in complex, dynamic, and uncertain environments."}),"\n",(0,i.jsx)(n.h2,{id:"the-reinforcement-learning-framework",children:"The Reinforcement Learning Framework"}),"\n",(0,i.jsx)(n.p,{children:"The RL framework consists of the following components:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Agent"}),": The learner or decision-maker."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Environment"}),": The world in which the agent exists and interacts."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"State (s)"}),": A representation of the current situation of the agent in the environment."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Action (a)"}),": A decision made by the agent."]}),"\n"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Reward (r)"}),": A feedback signal that the agent receives from the environment after taking an action."]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:["The goal of the agent is to learn a ",(0,i.jsx)(n.strong,{children:"policy (\u03c0)"}),", which is a mapping from states to actions, that maximizes the expected cumulative reward."]}),"\n",(0,i.jsx)(n.h2,{id:"q-learning",children:"Q-Learning"}),"\n",(0,i.jsx)(n.p,{children:"Q-Learning is a model-free reinforcement learning algorithm to learn the value of an action in a particular state. It does not require a model of the environment and can handle problems with stochastic transitions and rewards, without requiring adaptations."}),"\n",(0,i.jsxs)(n.p,{children:["The Q-function ",(0,i.jsx)(n.code,{children:"Q(s, a)"})," represents the expected future reward for taking action ",(0,i.jsx)(n.code,{children:"a"})," in state ",(0,i.jsx)(n.code,{children:"s"})," and following an optimal policy thereafter. The Q-function is updated using the Bellman equation:"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.code,{children:"Q(s, a) \u2190 Q(s, a) + \u03b1 * (r + \u03b3 * max(Q(s', a')) - Q(s, a))"})}),"\n",(0,i.jsx)(n.p,{children:"Where:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"\u03b1"})," is the learning rate."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"\u03b3"})," is the discount factor."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"s'"})," is the new state."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"a'"})," is the new action."]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"code-example-simple-q-learning-in-python",children:"Code Example: Simple Q-Learning in Python"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"import numpy as np\r\n\r\n# Initialize Q-table\r\nq_table = np.zeros([num_states, num_actions])\r\n\r\n# Hyperparameters\r\nalpha = 0.1\r\ngamma = 0.6\r\nepsilon = 0.1\r\n\r\n# For plotting metrics\r\nall_epochs = []\r\nall_penalties = []\r\n\r\nfor i in range(1, 100001):\r\n    state = env.reset()\r\n\r\n    epochs, penalties, reward, = 0, 0, 0\r\n    done = False\r\n    \r\n    while not done:\r\n        if random.uniform(0, 1) < epsilon:\r\n            action = env.action_space.sample() # Explore action space\r\n        else:\r\n            action = np.argmax(q_table[state]) # Exploit learned values\r\n\r\n        next_state, reward, done, info = env.step(action) \r\n        \r\n        old_value = q_table[state, action]\r\n        next_max = np.max(q_table[next_state])\r\n        \r\n        new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\r\n        q_table[state, action] = new_value\r\n\r\n        if reward == -10:\r\n            penalties += 1\r\n\r\n        state = next_state\r\n        epochs += 1\n"})}),"\n",(0,i.jsx)(n.h2,{id:"deep-reinforcement-learning",children:"Deep Reinforcement Learning"}),"\n",(0,i.jsx)(n.p,{children:"For problems with large state and action spaces, it is not feasible to represent the Q-function as a table. In these cases, we can use a deep neural network to approximate the Q-function. This is known as Deep Reinforcement Learning (DRL)."}),"\n",(0,i.jsx)(n.h3,{id:"references",children:"References"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGXMLuzthVKnPuIu748FiYkH3nhy-UBCz84xrd-MIouEtXIy41riI3fYx1pqkxuwnjeQKDsQXmnQ7RHT-WXEnjjrPLp1iytXFhDgoTC8ueeECmvnSv5zR-eiecCObrhQSaE0Xx3c10DzWEXRhKGDJjwFSb1HV0uClec8L7qeDdbQB4=",children:"Substack - Physical AI and Reinforcement Learning"})}),"\n"]})]})}function d(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(h,{...e})}):h(e)}},8453:(e,n,r)=>{r.d(n,{R:()=>s,x:()=>o});var t=r(6540);const i={},a=t.createContext(i);function s(e){const n=t.useContext(a);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:s(e.components),t.createElement(a.Provider,{value:n},e.children)}}}]);